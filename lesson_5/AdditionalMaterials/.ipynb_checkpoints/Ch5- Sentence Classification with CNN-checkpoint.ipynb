{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose= verbose\n",
    "        self.max_sentence_len= 0\n",
    "        self.questions= list()\n",
    "        self.str_labels= list()\n",
    "        self.numeral_labels= list()\n",
    "        self.numeral_data= list()\n",
    "        self.cur_pos=0\n",
    "        \n",
    "    def maybe_download(self, dir_name, file_name, url):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if self.verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_name):\n",
    "        file_path= os.path.join(dir_name, file_name)\n",
    "        self.questions= list(); self.labels= list()\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "            for row in f:\n",
    "                row_str= row.split(\":\")\n",
    "                label, question= row_str[0], row_str[1]\n",
    "                question= question.lower()\n",
    "                self.labels.append(label)\n",
    "                self.questions.append(question.split())\n",
    "                if self.max_sentence_len < len(self.questions[-1]):\n",
    "                    self.max_sentence_len= len(self.questions[-1])\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.labels)\n",
    "        self.numeral_labels = le.transform(self.labels)\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"Sample questions \\n\")\n",
    "            print(self.questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def padding(self, length):\n",
    "        for question in self.questions:\n",
    "            question= question.extend([\"pad\"]*(length- len(question)))\n",
    "    \n",
    "    def build_numeral_data(self, dictionary):\n",
    "        self.numeral_data= list()\n",
    "        for question in self.questions:\n",
    "            data= list()\n",
    "            for word in question:\n",
    "                data.append(dictionary[word])\n",
    "            self.numeral_data.append(data)\n",
    "        if self.verbose:\n",
    "            print('Sample numeral data \\n')   \n",
    "            print(self.numeral_data[0:5])\n",
    "    \n",
    "    def train_valid_split(self, train_size=0.9, rand_seed=33):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(np.array(self.numeral_data), np.array(self.numeral_labels), \n",
    "                                                            test_size = 1-train_size, random_state= rand_seed)\n",
    "        self.train_numeral= X_train\n",
    "        self.train_labels= y_train\n",
    "        self.valid_numeral= X_valid\n",
    "        self.valid_labels= y_valid\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_dictionary_count(questions):\n",
    "        count= []\n",
    "        dictionary= dict()\n",
    "        words= []\n",
    "        for question in questions:\n",
    "            words.extend(question)\n",
    "        count.extend(collections.Counter(words).most_common())\n",
    "        for word,freq in count:\n",
    "            dictionary[word]= len(dictionary)\n",
    "        reverse_dictionary= dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        return dictionary, reverse_dictionary, count\n",
    "    \n",
    "    def next_batch(self, batch_size, vocab_size, input_len):\n",
    "        data_batch= np.zeros([batch_size, input_len, vocab_size])\n",
    "        label_batch= np.zeros([batch_size, self.num_classes])\n",
    "        train_size= len(self.train_numeral)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(input_len):\n",
    "                data_batch[i,j, self.train_numeral[self.cur_pos][j]]=1\n",
    "            label_batch[i, self.train_labels[self.cur_pos]]=1\n",
    "            self.cur_pos= (self.cur_pos+1)%train_size\n",
    "        return data_batch, label_batch\n",
    "    \n",
    "    def convert_to_feed(self, data_numeral, label_numeral, input_len, vocab_size):\n",
    "        data2feed= np.zeros([data_numeral.shape[0], input_len, vocab_size])\n",
    "        label2feed= np.zeros([data_numeral.shape[0], self.num_classes])\n",
    "        for i in range(data_numeral.shape[0]):\n",
    "            for j in range(input_len):\n",
    "                data2feed[i,j, data_numeral[i][j]]=1\n",
    "            label2feed[i, label_numeral[i]]=1\n",
    "        return data2feed, label2feed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Data/question-classif-data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7465f85fc486>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_dm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_dm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/question-classif-data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train_1000.label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"http://cogcomp.org/Data/QA/QC/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_dm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_dm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/question-classif-data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TREC_10.label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"http://cogcomp.org/Data/QA/QC/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-281eb2e94205>\u001b[0m in \u001b[0;36mmaybe_download\u001b[1;34m(self, dir_name, file_name, url)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Data/question-classif-data'"
     ]
    }
   ],
   "source": [
    "train_dm = DataManager()\n",
    "train_dm.maybe_download(\"Data/question-classif-data\", \"train_1000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "test_dm = DataManager()\n",
    "test_dm.maybe_download(\"Data/question-classif-data\", \"TREC_10.label\", \"http://cogcomp.org/Data/QA/QC/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample questions \n",
      "\n",
      "[['manner', 'how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?'], ['cremat', 'what', 'films', 'featured', 'the', 'character', 'popeye', 'doyle', '?'], ['manner', 'how', 'can', 'i', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?'], ['animal', 'what', 'fowl', 'grabs', 'the', 'spotlight', 'after', 'the', 'chinese', 'year', 'of', 'the', 'monkey', '?'], ['exp', 'what', 'is', 'the', 'full', 'form', 'of', '.com', '?']]\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n",
      "Sample questions \n",
      "\n",
      "[['dist', 'how', 'far', 'is', 'it', 'from', 'denver', 'to', 'aspen', '?'], ['city', 'what', 'county', 'is', 'modesto', ',', 'california', 'in', '?'], ['desc', 'who', 'was', 'galileo', '?'], ['def', 'what', 'is', 'an', 'atom', '?'], ['date', 'when', 'did', 'hawaii', 'become', 'a', 'state', '?']]\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n",
      "Sample numeral data \n",
      "\n",
      "[[38, 12, 19, 1006, 1007, 6, 28, 1008, 1009, 544, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [44, 3, 545, 1010, 2, 163, 1011, 1012, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [38, 12, 47, 42, 63, 8, 380, 5, 1013, 88, 294, 164, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [37, 3, 1014, 1015, 2, 1016, 233, 2, 546, 53, 5, 2, 1017, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [64, 3, 4, 2, 381, 234, 5, 1018, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Sample numeral data \n",
      "\n",
      "[[74, 12, 197, 4, 72, 39, 2841, 16, 2842, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [26, 3, 259, 4, 2843, 20, 455, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [33, 13, 11, 2844, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 3, 4, 54, 2845, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 30, 19, 946, 114, 8, 36, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Sample data batch- label batch \n",
      "\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_dm.read_data(\"Data/question-classif-data\", \"train_1000.label\")\n",
    "test_dm.read_data(\"Data/question-classif-data\", \"TREC_10.label\")\n",
    "pad_len = max(train_dm.max_sentence_len, test_dm.max_sentence_len)\n",
    "train_dm.padding(pad_len)\n",
    "test_dm.padding(pad_len)\n",
    "all_questions= list(train_dm.questions) \n",
    "all_questions.extend(test_dm.questions)\n",
    "dictionary,_,_= DataManager.build_dictionary_count(all_questions)\n",
    "train_dm.build_numeral_data(dictionary)\n",
    "test_dm.build_numeral_data(dictionary)\n",
    "train_dm.train_valid_split()\n",
    "data_batch, label_batch= train_dm.next_batch(batch_size=5, vocab_size= len(dictionary), input_len= pad_len)\n",
    "print(\"Sample data batch- label batch \\n\")\n",
    "print(data_batch)\n",
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    @staticmethod\n",
    "    def dense(inputs, output_size, name=\"dense1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            input_size= int(inputs.get_shape()[1])\n",
    "            W_init = tf.random_normal([input_size, output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "            b_init= tf.random_normal([output_size], mean=0, stddev= 0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name= \"W\")\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.matmul(inputs, W) + b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv2D(inputs, filter_shape, strides=[1,1,1,1], padding=\"SAME\", name= \"conv1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            W_init= tf.random_normal(filter_shape, mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name=\"W\")\n",
    "            b_init= tf.random_normal([int(filter_shape[3])], mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.nn.conv2d(input= inputs, filter= W, strides= strides, padding= padding)+b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "            \n",
    "    @staticmethod\n",
    "    def conv1D(inputs, filter_shape, stride=1, padding=\"SAME\", name=\"conv1\", act=None):\n",
    "        with tf.name_scope(name):\n",
    "            W_init= tf.random_normal(filter_shape, mean=0, stddev=0.1, dtype= tf.float32)\n",
    "            W= tf.Variable(W_init, name=\"W\")\n",
    "            b_init= tf.random_normal([filter_shape[2]], mean=0, stddev=0.1)\n",
    "            b= tf.Variable(b_init, name=\"b\")\n",
    "            Wxb= tf.nn.conv1d(value=inputs, filters=W, stride= stride, padding= padding) +b\n",
    "            if act is None:\n",
    "                return Wxb\n",
    "            else:\n",
    "                return act(Wxb)\n",
    "    \n",
    "    @staticmethod\n",
    "    def max_pool(inputs, ksize=[1,2,2,1],strides=[1,2,2,1], padding=\"SAME\"):\n",
    "        return tf.nn.max_pool(value= inputs, ksize=ksize, strides= strides, padding= padding)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout(inputs, keep_prob):\n",
    "        return tf.nn.dropout(inputs, keep_prob= keep_prob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_norm(inputs, phase_train):\n",
    "        return tf.contrib.layers.batch_norm(inputs, decay= 0.99, \n",
    "                                            is_training=phase_train, center= True, scale=True, reuse= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SC_CNN:\n",
    "    def __init__(self, height, width, batch_size=32, epochs=100, num_classes=5, save_history= True, \n",
    "                 verbose= True, optimizer= tf.train.AdamOptimizer(learning_rate=0.001), learning_rate=0.001):\n",
    "        tf.reset_default_graph()\n",
    "        self.height= height \n",
    "        self.width= width\n",
    "        self.batch_size= batch_size\n",
    "        self.epochs= epochs\n",
    "        self.num_classes= num_classes\n",
    "        self.optimizer= optimizer\n",
    "        self.optimizer.learning_rate= learning_rate\n",
    "        self.verbose= verbose\n",
    "        self.save_history= save_history\n",
    "        if self.save_history:\n",
    "            self.H= {\"train_loss_batch\": [], \"train_acc_batch\": [], \"train_loss_epoch\": [], \n",
    "                     \"train_acc_epoch\": [], \"valid_loss_epoch\": [], \"valid_acc_epoch\": []}\n",
    "        self.session= tf.Session()\n",
    "    \n",
    "    def build(self):\n",
    "        self.X= tf.placeholder(shape= [None, self.height, self.width], dtype=tf.float32)\n",
    "        self.y= tf.placeholder(shape= [None, self.num_classes], dtype= tf.float32)\n",
    "        conv1= Layers.conv1D(inputs= self.X, filter_shape= [3, self.width, 1], name=\"conv1\")\n",
    "        conv2= Layers.conv1D(inputs= self.X, filter_shape= [5, self.width, 1], name=\"conv2\")\n",
    "        conv3= Layers.conv1D(inputs= self.X, filter_shape= [7, self.width, 1], name=\"conv3\")\n",
    "        h1=tf.reduce_max(conv1, axis=1)\n",
    "        h2=tf.reduce_max(conv2, axis=1)\n",
    "        h3= tf.reduce_max(conv3, axis=1)\n",
    "        h= tf.concat([h1,h2,h3], axis=1)\n",
    "        logits= Layers.dense(inputs= h, output_size= self.num_classes)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            cross_entropy= tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits= logits)\n",
    "            self.loss= tf.reduce_mean(cross_entropy)\n",
    "            self.train= self.optimizer.minimize(self.loss)\n",
    "        with tf.name_scope(\"predict\"):\n",
    "            self.y_pred= tf.argmax(logits, axis=1)\n",
    "            y1= tf.argmax(self.y, axis=1)\n",
    "            corrections= tf.cast(tf.equal(self.y_pred, y1), dtype=tf.float32)\n",
    "            self.accuracy= tf.reduce_mean(corrections)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def partial_fit(self, data_batch, label_batch):\n",
    "        self.session.run([self.train], feed_dict={self.X:data_batch, self.y:label_batch})\n",
    "        if self.save_history:\n",
    "            self.compute_loss_acc(data_batch, label_batch, \"train\", \"Iteration\", 1)\n",
    "    \n",
    "    def predict(self, X,y):\n",
    "        y_pred, acc= self.session.run([self.y_pred, self.accuracy], feed_dict={self.X:X, self.y:y})\n",
    "        return y_pred, acc\n",
    "    \n",
    "    def compute_loss_acc(self, X, y, applied_set=\"train\", applied_scope=\"Epoch\", index= 1):\n",
    "        loss, acc= self.session.run([self.loss, self.accuracy], feed_dict={self.X:X, self.y:y})\n",
    "        if self.verbose and applied_scope==\"Epoch\":\n",
    "            print(\"{} {} {} loss= {}, acc={}\".format(applied_scope, index, applied_set, loss, acc))\n",
    "        if self.save_history:\n",
    "            if applied_scope==\"Iteration\":\n",
    "                self.H[\"train_loss_batch\"].append(loss)\n",
    "                self.H[\"train_acc_batch\"].append(acc)\n",
    "            else:\n",
    "                if applied_set==\"train\":\n",
    "                    self.H[\"train_loss_epoch\"].append(loss)\n",
    "                    self.H[\"train_acc_epoch\"].append(acc)\n",
    "                else:\n",
    "                    self.H[\"valid_loss_epoch\"].append(loss)\n",
    "                    self.H[\"valid_acc_epoch\"].append(acc)    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 33, 3369)\n",
      "Epoch 1 train loss= 1.764917016029358, acc=0.24444444477558136\n",
      "Epoch 1 valid loss= 1.7809503078460693, acc=0.25999999046325684\n",
      "Epoch 2 train loss= 1.745826244354248, acc=0.2455555498600006\n",
      "Epoch 2 valid loss= 1.7620246410369873, acc=0.25\n",
      "Epoch 3 train loss= 1.7264493703842163, acc=0.24444444477558136\n",
      "Epoch 3 valid loss= 1.7437671422958374, acc=0.23999999463558197\n",
      "Epoch 4 train loss= 1.7061278820037842, acc=0.24444444477558136\n",
      "Epoch 4 valid loss= 1.7255263328552246, acc=0.23999999463558197\n",
      "Epoch 5 train loss= 1.684456467628479, acc=0.24444444477558136\n",
      "Epoch 5 valid loss= 1.7070801258087158, acc=0.23999999463558197\n",
      "Epoch 6 train loss= 1.6611368656158447, acc=0.24444444477558136\n",
      "Epoch 6 valid loss= 1.6876728534698486, acc=0.23999999463558197\n",
      "Epoch 7 train loss= 1.635827898979187, acc=0.24444444477558136\n",
      "Epoch 7 valid loss= 1.6672416925430298, acc=0.23999999463558197\n",
      "Epoch 8 train loss= 1.607904076576233, acc=0.24444444477558136\n",
      "Epoch 8 valid loss= 1.6451866626739502, acc=0.25\n",
      "Epoch 9 train loss= 1.5767650604248047, acc=0.27444443106651306\n",
      "Epoch 9 valid loss= 1.6214568614959717, acc=0.27000001072883606\n",
      "Epoch 10 train loss= 1.5418951511383057, acc=0.4000000059604645\n",
      "Epoch 10 valid loss= 1.5955173969268799, acc=0.3400000035762787\n",
      "Epoch 11 train loss= 1.5032230615615845, acc=0.4933333396911621\n",
      "Epoch 11 valid loss= 1.5664496421813965, acc=0.3799999952316284\n",
      "Epoch 12 train loss= 1.4609500169754028, acc=0.5688889026641846\n",
      "Epoch 12 valid loss= 1.5343807935714722, acc=0.4099999964237213\n",
      "Epoch 13 train loss= 1.415968656539917, acc=0.605555534362793\n",
      "Epoch 13 valid loss= 1.499000906944275, acc=0.47999998927116394\n",
      "Epoch 14 train loss= 1.3686457872390747, acc=0.6311110854148865\n",
      "Epoch 14 valid loss= 1.4619393348693848, acc=0.49000000953674316\n",
      "Epoch 15 train loss= 1.320029854774475, acc=0.6577777862548828\n",
      "Epoch 15 valid loss= 1.4231441020965576, acc=0.5600000023841858\n",
      "Epoch 16 train loss= 1.2699834108352661, acc=0.6800000071525574\n",
      "Epoch 16 valid loss= 1.3835948705673218, acc=0.5799999833106995\n",
      "Epoch 17 train loss= 1.21916925907135, acc=0.7077777981758118\n",
      "Epoch 17 valid loss= 1.3434972763061523, acc=0.5899999737739563\n",
      "Epoch 18 train loss= 1.1687753200531006, acc=0.7222222089767456\n",
      "Epoch 18 valid loss= 1.3033404350280762, acc=0.6200000047683716\n",
      "Epoch 19 train loss= 1.1193280220031738, acc=0.7333333492279053\n",
      "Epoch 19 valid loss= 1.2638053894042969, acc=0.6399999856948853\n",
      "Epoch 20 train loss= 1.0713975429534912, acc=0.7522222399711609\n",
      "Epoch 20 valid loss= 1.2257919311523438, acc=0.6700000166893005\n",
      "Epoch 21 train loss= 1.025052547454834, acc=0.757777750492096\n",
      "Epoch 21 valid loss= 1.1889350414276123, acc=0.6800000071525574\n",
      "Epoch 22 train loss= 0.9804467558860779, acc=0.7622222304344177\n",
      "Epoch 22 valid loss= 1.1532926559448242, acc=0.6899999976158142\n",
      "Epoch 23 train loss= 0.9380720853805542, acc=0.7733333110809326\n",
      "Epoch 23 valid loss= 1.1192517280578613, acc=0.699999988079071\n",
      "Epoch 24 train loss= 0.8974964022636414, acc=0.7833333611488342\n",
      "Epoch 24 valid loss= 1.0858389139175415, acc=0.699999988079071\n",
      "Epoch 25 train loss= 0.8588241934776306, acc=0.7922222018241882\n",
      "Epoch 25 valid loss= 1.0546314716339111, acc=0.7200000286102295\n",
      "Epoch 26 train loss= 0.8221638202667236, acc=0.7977777719497681\n",
      "Epoch 26 valid loss= 1.0250316858291626, acc=0.7200000286102295\n",
      "Epoch 27 train loss= 0.7871260046958923, acc=0.8055555820465088\n",
      "Epoch 27 valid loss= 0.9971956014633179, acc=0.7400000095367432\n",
      "Epoch 28 train loss= 0.753535807132721, acc=0.8088889122009277\n",
      "Epoch 28 valid loss= 0.9696077108383179, acc=0.75\n",
      "Epoch 29 train loss= 0.721443772315979, acc=0.8122222423553467\n",
      "Epoch 29 valid loss= 0.9434594511985779, acc=0.75\n",
      "Epoch 30 train loss= 0.6904770731925964, acc=0.8199999928474426\n",
      "Epoch 30 valid loss= 0.9188359975814819, acc=0.75\n",
      "Epoch 31 train loss= 0.6608061194419861, acc=0.8211110830307007\n",
      "Epoch 31 valid loss= 0.8941553235054016, acc=0.75\n",
      "Epoch 32 train loss= 0.6321656107902527, acc=0.8322222232818604\n",
      "Epoch 32 valid loss= 0.8704918622970581, acc=0.75\n",
      "Epoch 33 train loss= 0.6046769022941589, acc=0.851111114025116\n",
      "Epoch 33 valid loss= 0.8475178480148315, acc=0.75\n",
      "Epoch 34 train loss= 0.5781165361404419, acc=0.8622221946716309\n",
      "Epoch 34 valid loss= 0.8249186873435974, acc=0.75\n",
      "Epoch 35 train loss= 0.5525093674659729, acc=0.8744444251060486\n",
      "Epoch 35 valid loss= 0.8036039471626282, acc=0.75\n",
      "Epoch 36 train loss= 0.5278341174125671, acc=0.902222216129303\n",
      "Epoch 36 valid loss= 0.7821781039237976, acc=0.75\n",
      "Epoch 37 train loss= 0.5039999485015869, acc=0.9244444370269775\n",
      "Epoch 37 valid loss= 0.7611307501792908, acc=0.75\n",
      "Epoch 38 train loss= 0.48113858699798584, acc=0.9333333373069763\n",
      "Epoch 38 valid loss= 0.7409815788269043, acc=0.75\n",
      "Epoch 39 train loss= 0.45907798409461975, acc=0.9444444179534912\n",
      "Epoch 39 valid loss= 0.7210597991943359, acc=0.7699999809265137\n",
      "Epoch 40 train loss= 0.43790045380592346, acc=0.9466666579246521\n",
      "Epoch 40 valid loss= 0.7017192840576172, acc=0.7900000214576721\n",
      "Epoch 41 train loss= 0.41755789518356323, acc=0.9555555582046509\n",
      "Epoch 41 valid loss= 0.6829400658607483, acc=0.8199999928474426\n",
      "Epoch 42 train loss= 0.39808714389801025, acc=0.9577777981758118\n",
      "Epoch 42 valid loss= 0.6644905805587769, acc=0.8299999833106995\n",
      "Epoch 43 train loss= 0.3796399235725403, acc=0.9588888883590698\n",
      "Epoch 43 valid loss= 0.647057294845581, acc=0.8399999737739563\n",
      "Epoch 44 train loss= 0.3620207607746124, acc=0.9599999785423279\n",
      "Epoch 44 valid loss= 0.6298662424087524, acc=0.8700000047683716\n",
      "Epoch 45 train loss= 0.34548965096473694, acc=0.9622222185134888\n",
      "Epoch 45 valid loss= 0.6131483316421509, acc=0.8700000047683716\n",
      "Epoch 46 train loss= 0.3297300338745117, acc=0.9644444584846497\n",
      "Epoch 46 valid loss= 0.597772479057312, acc=0.8799999952316284\n",
      "Epoch 47 train loss= 0.314933717250824, acc=0.9644444584846497\n",
      "Epoch 47 valid loss= 0.5821241140365601, acc=0.8799999952316284\n",
      "Epoch 48 train loss= 0.30102628469467163, acc=0.9655555486679077\n",
      "Epoch 48 valid loss= 0.5677244663238525, acc=0.8799999952316284\n",
      "Epoch 49 train loss= 0.28789442777633667, acc=0.9655555486679077\n",
      "Epoch 49 valid loss= 0.553640604019165, acc=0.8799999952316284\n",
      "Epoch 50 train loss= 0.27563777565956116, acc=0.9677777886390686\n",
      "Epoch 50 valid loss= 0.5402789115905762, acc=0.8999999761581421\n",
      "Epoch 51 train loss= 0.264104425907135, acc=0.9677777886390686\n",
      "Epoch 51 valid loss= 0.527012825012207, acc=0.9100000262260437\n",
      "Epoch 52 train loss= 0.2533143162727356, acc=0.9688888788223267\n",
      "Epoch 52 valid loss= 0.5148091316223145, acc=0.9100000262260437\n",
      "Epoch 53 train loss= 0.24324458837509155, acc=0.9688888788223267\n",
      "Epoch 53 valid loss= 0.5027801394462585, acc=0.9100000262260437\n",
      "Epoch 54 train loss= 0.23377977311611176, acc=0.9688888788223267\n",
      "Epoch 54 valid loss= 0.49153271317481995, acc=0.9200000166893005\n",
      "Epoch 55 train loss= 0.22488267719745636, acc=0.9688888788223267\n",
      "Epoch 55 valid loss= 0.48041805624961853, acc=0.9200000166893005\n",
      "Epoch 56 train loss= 0.21651853621006012, acc=0.9700000286102295\n",
      "Epoch 56 valid loss= 0.4697969853878021, acc=0.9200000166893005\n",
      "Epoch 57 train loss= 0.2087482064962387, acc=0.9700000286102295\n",
      "Epoch 57 valid loss= 0.4596748650074005, acc=0.9200000166893005\n",
      "Epoch 58 train loss= 0.20138683915138245, acc=0.9700000286102295\n",
      "Epoch 58 valid loss= 0.44974473118782043, acc=0.9200000166893005\n",
      "Epoch 59 train loss= 0.19444061815738678, acc=0.9700000286102295\n",
      "Epoch 59 valid loss= 0.44048312306404114, acc=0.9200000166893005\n",
      "Epoch 60 train loss= 0.1879405379295349, acc=0.9700000286102295\n",
      "Epoch 60 valid loss= 0.4315263032913208, acc=0.9200000166893005\n",
      "Epoch 61 train loss= 0.181815966963768, acc=0.9722222089767456\n",
      "Epoch 61 valid loss= 0.42292967438697815, acc=0.9200000166893005\n",
      "Epoch 62 train loss= 0.1760595589876175, acc=0.9722222089767456\n",
      "Epoch 62 valid loss= 0.4147111475467682, acc=0.9200000166893005\n",
      "Epoch 63 train loss= 0.1706119328737259, acc=0.9722222089767456\n",
      "Epoch 63 valid loss= 0.4069400131702423, acc=0.9200000166893005\n",
      "Epoch 64 train loss= 0.16546909511089325, acc=0.9733333587646484\n",
      "Epoch 64 valid loss= 0.3991829752922058, acc=0.9200000166893005\n",
      "Epoch 65 train loss= 0.1606339067220688, acc=0.9744444489479065\n",
      "Epoch 65 valid loss= 0.392289400100708, acc=0.9200000166893005\n",
      "Epoch 66 train loss= 0.1560349315404892, acc=0.9744444489479065\n",
      "Epoch 66 valid loss= 0.384985089302063, acc=0.9200000166893005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 train loss= 0.15165923535823822, acc=0.9744444489479065\n",
      "Epoch 67 valid loss= 0.37838059663772583, acc=0.9200000166893005\n",
      "Epoch 68 train loss= 0.1474999338388443, acc=0.9744444489479065\n",
      "Epoch 68 valid loss= 0.3721283972263336, acc=0.9300000071525574\n",
      "Epoch 69 train loss= 0.14356715977191925, acc=0.9744444489479065\n",
      "Epoch 69 valid loss= 0.36593109369277954, acc=0.9300000071525574\n",
      "Epoch 70 train loss= 0.13980162143707275, acc=0.9744444489479065\n",
      "Epoch 70 valid loss= 0.3599025011062622, acc=0.9300000071525574\n",
      "Epoch 71 train loss= 0.1362338662147522, acc=0.9744444489479065\n",
      "Epoch 71 valid loss= 0.3543422222137451, acc=0.9300000071525574\n",
      "Epoch 72 train loss= 0.13280978798866272, acc=0.9744444489479065\n",
      "Epoch 72 valid loss= 0.34881383180618286, acc=0.9300000071525574\n",
      "Epoch 73 train loss= 0.12957368791103363, acc=0.9744444489479065\n",
      "Epoch 73 valid loss= 0.3434177041053772, acc=0.9300000071525574\n",
      "Epoch 74 train loss= 0.12643450498580933, acc=0.9744444489479065\n",
      "Epoch 74 valid loss= 0.3386068642139435, acc=0.9300000071525574\n",
      "Epoch 75 train loss= 0.12337229400873184, acc=0.9744444489479065\n",
      "Epoch 75 valid loss= 0.33368828892707825, acc=0.9300000071525574\n",
      "Epoch 76 train loss= 0.12046118825674057, acc=0.9744444489479065\n",
      "Epoch 76 valid loss= 0.3290930986404419, acc=0.9399999976158142\n",
      "Epoch 77 train loss= 0.11767115443944931, acc=0.9744444489479065\n",
      "Epoch 77 valid loss= 0.324214905500412, acc=0.9399999976158142\n",
      "Epoch 78 train loss= 0.11499197781085968, acc=0.9744444489479065\n",
      "Epoch 78 valid loss= 0.31989169120788574, acc=0.9399999976158142\n",
      "Epoch 79 train loss= 0.11241334676742554, acc=0.9744444489479065\n",
      "Epoch 79 valid loss= 0.31544116139411926, acc=0.9399999976158142\n",
      "Epoch 80 train loss= 0.10991775989532471, acc=0.9744444489479065\n",
      "Epoch 80 valid loss= 0.3114391565322876, acc=0.9399999976158142\n",
      "Epoch 81 train loss= 0.10750185698270798, acc=0.9744444489479065\n",
      "Epoch 81 valid loss= 0.3072659969329834, acc=0.9399999976158142\n",
      "Epoch 82 train loss= 0.1051730141043663, acc=0.9744444489479065\n",
      "Epoch 82 valid loss= 0.30331000685691833, acc=0.9399999976158142\n",
      "Epoch 83 train loss= 0.10292287170886993, acc=0.9744444489479065\n",
      "Epoch 83 valid loss= 0.29923009872436523, acc=0.9399999976158142\n",
      "Epoch 84 train loss= 0.10075175762176514, acc=0.9744444489479065\n",
      "Epoch 84 valid loss= 0.295746386051178, acc=0.9399999976158142\n",
      "Epoch 85 train loss= 0.09864912182092667, acc=0.9744444489479065\n",
      "Epoch 85 valid loss= 0.2920666038990021, acc=0.9399999976158142\n",
      "Epoch 86 train loss= 0.09659850597381592, acc=0.9744444489479065\n",
      "Epoch 86 valid loss= 0.28875958919525146, acc=0.9399999976158142\n",
      "Epoch 87 train loss= 0.09461283683776855, acc=0.9744444489479065\n",
      "Epoch 87 valid loss= 0.2848501205444336, acc=0.9399999976158142\n",
      "Epoch 88 train loss= 0.09265836328268051, acc=0.9744444489479065\n",
      "Epoch 88 valid loss= 0.2815110683441162, acc=0.9399999976158142\n",
      "Epoch 89 train loss= 0.09076616913080215, acc=0.9744444489479065\n",
      "Epoch 89 valid loss= 0.27828487753868103, acc=0.9399999976158142\n",
      "Epoch 90 train loss= 0.0889168381690979, acc=0.9755555391311646\n",
      "Epoch 90 valid loss= 0.27481022477149963, acc=0.9399999976158142\n",
      "Epoch 91 train loss= 0.08713364601135254, acc=0.9755555391311646\n",
      "Epoch 91 valid loss= 0.2717106342315674, acc=0.9399999976158142\n",
      "Epoch 92 train loss= 0.08538969606161118, acc=0.9755555391311646\n",
      "Epoch 92 valid loss= 0.2686900496482849, acc=0.9399999976158142\n",
      "Epoch 93 train loss= 0.08368176221847534, acc=0.9755555391311646\n",
      "Epoch 93 valid loss= 0.2656346261501312, acc=0.9399999976158142\n",
      "Epoch 94 train loss= 0.08200225979089737, acc=0.9755555391311646\n",
      "Epoch 94 valid loss= 0.2626376450061798, acc=0.9399999976158142\n",
      "Epoch 95 train loss= 0.08035591244697571, acc=0.9755555391311646\n",
      "Epoch 95 valid loss= 0.259738564491272, acc=0.9399999976158142\n",
      "Epoch 96 train loss= 0.07872060686349869, acc=0.9822221994400024\n",
      "Epoch 96 valid loss= 0.2567581236362457, acc=0.9399999976158142\n",
      "Epoch 97 train loss= 0.07715771347284317, acc=0.9822221994400024\n",
      "Epoch 97 valid loss= 0.25387799739837646, acc=0.9399999976158142\n",
      "Epoch 98 train loss= 0.07560139894485474, acc=0.9833333492279053\n",
      "Epoch 98 valid loss= 0.25126466155052185, acc=0.9399999976158142\n",
      "Epoch 99 train loss= 0.0740845650434494, acc=0.9844444394111633\n",
      "Epoch 99 valid loss= 0.24855776131153107, acc=0.9399999976158142\n",
      "Epoch 100 train loss= 0.07261859625577927, acc=0.9855555295944214\n",
      "Epoch 100 valid loss= 0.2462541162967682, acc=0.9399999976158142\n",
      "Finish training and computing testing performance\n",
      "\n",
      "Testing accuracy: 0.9160000085830688\n"
     ]
    }
   ],
   "source": [
    "batch_size= 32\n",
    "epochs= 100\n",
    "train_size= len(train_dm.train_numeral)\n",
    "iter_per_epoch= math.ceil(train_size/batch_size)\n",
    "network= SC_CNN(height= pad_len, width= len(dictionary),batch_size=batch_size, epochs= epochs, num_classes= train_dm.num_classes)\n",
    "network.build()\n",
    "\n",
    "train2feed, train_label2feed= train_dm.convert_to_feed(train_dm.train_numeral, train_dm.train_labels, \n",
    "                                     input_len= pad_len, vocab_size=len(dictionary))\n",
    "\n",
    "valid2feed, valid_label2feed=  train_dm.convert_to_feed(train_dm.valid_numeral, train_dm.valid_labels, \n",
    "                                     input_len= pad_len, vocab_size=len(dictionary))\n",
    "\n",
    "test2feed, test_label2feed= test_dm.convert_to_feed(np.array(test_dm.numeral_data), np.array(test_dm.numeral_labels), \n",
    "                                     input_len= pad_len, vocab_size=len(dictionary))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(iter_per_epoch):\n",
    "        data_batch, label_batch= train_dm.next_batch(batch_size= batch_size, \n",
    "                                                      vocab_size=len(dictionary), input_len= pad_len)\n",
    "        #print(data_batch.shape, label_batch.shape)\n",
    "        network.partial_fit(data_batch, label_batch)\n",
    "    network.compute_loss_acc(train2feed, train_label2feed, \"train\", \"Epoch\", epoch +1)\n",
    "    network.compute_loss_acc(valid2feed, valid_label2feed, \"valid\", \"Epoch\", epoch +1)\n",
    "print(\"Finish training and computing testing performance\\n\")\n",
    "y_pred, test_acc= network.predict(test2feed, test_label2feed)\n",
    "print(\"Testing accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ae038cae1ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mplot_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(history[\"train_loss_epoch\"], \"r^-\", label=\"train loss epoch\")\n",
    "    plt.plot(history[\"valid_loss_epoch\"], \"b*-\", label= \"valid loss epoch\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(history[\"train_acc_epoch\"], \"r^-\", label=\"train acc epoch\")\n",
    "    plt.plot(history[\"valid_acc_epoch\"], \"b*-\", label= \"valid acc epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(network.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
